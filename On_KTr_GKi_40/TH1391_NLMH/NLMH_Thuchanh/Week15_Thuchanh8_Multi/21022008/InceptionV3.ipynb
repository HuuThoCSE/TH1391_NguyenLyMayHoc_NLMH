{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"10qmeeMSjTRuNStKKA0rtN9zZqHOmSbba","authorship_tag":"ABX9TyOLpwaT1He8de+C0Ym6g0zL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p51kaxsewPIY","executionInfo":{"status":"ok","timestamp":1687840062824,"user_tz":-420,"elapsed":20009,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}},"outputId":"f99edef2-98bc-4d0e-cbcc-b7a46e201426"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# !unzip /content/drive/MyDrive/NLMH_Thuchanh/Week15_Thuchanh8/Dataset-Fruits-20230620.zip\n","# !unzip /content/apple.zip\n","# !unzip /content/banana.zip\n","# !unzip /content/orange.zip"],"metadata":{"id":"oOfsbxs6gwJQ","executionInfo":{"status":"ok","timestamp":1687840062825,"user_tz":-420,"elapsed":13,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# import os\n","# import glob\n","# import pandas as pd\n","# import xml.etree.ElementTree as ET\n","\n","# def xml_to_csv(path):\n","#     xml_list = []\n","#     for xml_file in glob.glob(path + '/*.xml'):\n","#         tree = ET.parse(xml_file)\n","#         root = tree.getroot()\n","#         for member in root.findall('object'):\n","#             bbx = member.find('bndbox')\n","#             xmin = int(bbx.find('xmin').text)\n","#             ymin = int(bbx.find('ymin').text)\n","#             xmax = int(bbx.find('xmax').text)\n","#             ymax = int(bbx.find('ymax').text)\n","#             label = member.find('name').text\n","\n","#             value = (root.find('filename').text,\n","#                      int(root.find('size')[0].text),\n","#                      int(root.find('size')[1].text),\n","#                      label,\n","#                      xmin,\n","#                      ymin,\n","#                      xmax,\n","#                      ymax\n","#                      )\n","#             xml_list.append(value)\n","#     column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n","#     xml_df = pd.DataFrame(xml_list, columns=column_name)\n","#     return xml_df\n","\n","# def main():\n","#     datasets = ['orange']\n","#     for ds in datasets:\n","#         xml_df = xml_to_csv(ds)\n","#         xml_df.to_csv('labels_{}.csv'.format(ds), index=None)\n","#         print('Successfully converted xml to csv.')\n","\n","# main()"],"metadata":{"id":"-Jy57tgWgBZf","executionInfo":{"status":"ok","timestamp":1687840062826,"user_tz":-420,"elapsed":14,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["**CÁC BƯỚC XÂY DỰNG MÔ HÌNH**\n","\n","Bước 1: Sử dụng VGG16 (được đào tạo trước trên ImageNet) và xóa đầu fully-connected (FC)\n","\n","Bước 2: Xây dựng lớp Fully connected (FC) mới\n","\n","Bước 3: Đặt lớp FC mới lên VGG16\n","\n","Bước 4: Tinh chỉnh toàn bộ mạng để phát hiện đối tượng\n","\n","**CẤU TRÚC THƯ MỤC**\n","\n","```\n","MSSV\n","├── dataset\n","│   ├── annotations\n","│   │   ├── apple.csv\n","│   │   ├── banana.csv\n","│   │   └── orange.csv\n","│   └── images\n","│       ├── apple [70 entries]\n","│       ├── banana [70 entries]\n","│       └── orange [70 entries]\n","└── output\n","    ├── plots\n","    │   ├── accs.png\n","    │   └── losses.png\n","    ├── detector.h5\n","    └── lb.pickle\n","```"],"metadata":{"id":"MxAATVPpeTaE"}},{"cell_type":"markdown","source":["# **THIẾT LẬP CÁC THAM SỐ**"],"metadata":{"id":"fWAWqXfVdBXG"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"cFQlJVmndBCq","executionInfo":{"status":"ok","timestamp":1687840062826,"user_tz":-420,"elapsed":13,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"outputs":[],"source":["# import the necessary packages\n","import os\n","# define the base path to the input dataset and then use it to derive\n","# the path to the input images and annotation CSV files\n","BASE_PATH = \"/content/drive/MyDrive/NLMH_Thuchanh/Week15_Thuchanh8/21022008/dataset\"\n","IMAGES_PATH = os.path.sep.join([BASE_PATH, \"images\"])\n","ANNOTS_PATH = os.path.sep.join([BASE_PATH, \"annotations\"])"]},{"cell_type":"code","source":["BASE_PATH"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"R5iQUvXMkiOw","executionInfo":{"status":"ok","timestamp":1687840062827,"user_tz":-420,"elapsed":14,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}},"outputId":"5e0cfb85-4f2e-419b-9a4f-8b7c107eae60"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/NLMH_Thuchanh/Week15_Thuchanh8/21022008/dataset'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# define the path to the base output directory\n","BASE_OUTPUT = \"/content/drive/MyDrive/NLMH_Thuchanh/Week15_Thuchanh8/21022008/output\"\n","# define the path to the output model, label binarizer, plots output\n","# directory, and testing image paths\n","MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector.h5\"])\n","LB_PATH = os.path.sep.join([BASE_OUTPUT, \"lb.pickle\"])\n","PLOTS_PATH = os.path.sep.join([BASE_OUTPUT, \"plots\"])\n","TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])"],"metadata":{"id":"QUzUmccidGOG","executionInfo":{"status":"ok","timestamp":1687840062828,"user_tz":-420,"elapsed":12,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# initialize our initial learning rate, number of epochs to train\n","# for, and the batch size\n","INIT_LR = 1e-4\n","NUM_EPOCHS = 50\n","BATCH_SIZE = 32"],"metadata":{"id":"kvWuGxwodJTA","executionInfo":{"status":"ok","timestamp":1687840062828,"user_tz":-420,"elapsed":11,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# **HUẤN LUYỆN MÔ HÌNH**"],"metadata":{"id":"OCPewI4ldKhg"}},{"cell_type":"markdown","source":["## **Import thư viện**"],"metadata":{"id":"gEBaZWF6dOsq"}},{"cell_type":"code","source":["# import the necessary packages\n","from tensorflow.keras.applications import InceptionV3\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.preprocessing import LabelBinarizer # Đơn giản\n","from sklearn.model_selection import train_test_split # Tách tập train, test\n","from sklearn.metrics import plot_confusion_matrix\n","from imutils import paths\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pickle\n","import cv2\n","import os"],"metadata":{"id":"vo5BbeihdMLM","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1687840066773,"user_tz":-420,"elapsed":3956,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}},"outputId":"335df915-6d47-44f7-995c-d0d029b1bdfa"},"execution_count":8,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-e62fbf9299d7>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelBinarizer\u001b[0m \u001b[0;31m# Đơn giản\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m \u001b[0;31m# Tách tập train, test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/usr/local/lib/python3.10/dist-packages/sklearn/metrics/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["## **Load dữ liệu**"],"metadata":{"id":"0UezGRZPdRsK"}},{"cell_type":"code","source":["# initialize the list of data (images), class labels, target bounding\n","# box coordinates, and image paths\n","\n","# 4 mãng\n","print(\"[INFO] loading dataset...\")\n","data = []\n","labels = []\n","bboxes = []\n","imagePaths = []"],"metadata":{"id":"d-3asdpTdQhf","executionInfo":{"status":"aborted","timestamp":1687840066774,"user_tz":-420,"elapsed":7,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loop over all CSV files in the annotations directory\n","for csvPath in paths.list_files(ANNOTS_PATH, validExts=(\".csv\")):\n","\t# load the contents of the current CSV annotations file\n","\trows = open(csvPath).read().strip().split(\"\\n\")\n","\t# loop over the rows\n","\tfor row in rows:\n","\t\t# break the row into the filename, bounding box coordinates,\n","\t\t# and class label\n","\t\trow = row.split(\",\")\n","\t\t(filename, startX, startY, endX, endY, label) = row\n","\t\t# derive the path to the input image, load the image (in\n","\t\t# OpenCV format), and grab its dimensions\n","\t\timagePath = os.path.sep.join([IMAGES_PATH, label,\n","\t\t\tfilename])\n","\t\timage = cv2.imread(imagePath)\n","\t\t(h, w) = image.shape[:2]\n","\t\t# scale the bounding box coordinates relative to the spatial\n","\t\t# dimensions of the input image\n","\t\t# Trích xuất tọa độ\n","\t\tstartX = float(startX) / w\n","\t\tstartY = float(startY) / h\n","\t\tendX = float(endX) / w\n","\t\tendY = float(endY) / h\n","\t\t# load the image and preprocess it\n","\t\timage = load_img(imagePath, target_size=(224, 224)) # Kích thước chuẩn hay sử dụng\n","\t\timage = img_to_array(image)\n","\t\t# update our list of data, class labels, bounding boxes, and\n","\t\t# image paths\n","\t\tdata.append(image) # [Lop] Thêm dữ liệu\n","\t\tlabels.append(label)\n","\t\tbboxes.append((startX, startY, endX, endY))\n","\t\timagePaths.append(imagePath)"],"metadata":{"id":"hiAyeXDYdVmm","executionInfo":{"status":"aborted","timestamp":1687840066774,"user_tz":-420,"elapsed":7,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Chuẩn hóa dữ liệu**"],"metadata":{"id":"w2-zmyas1m18"}},{"cell_type":"code","source":["# convert the data, class labels, bounding boxes, and image paths to\n","# NumPy arrays, scaling the input pixel intensities from the range\n","# [0, 255] to [0, 1]\n","data = np.array(data, dtype=\"float32\") / 255.0\n","labels = np.array(labels)\n","bboxes = np.array(bboxes, dtype=\"float32\")\n","imagePaths = np.array(imagePaths)\n","# perform one-hot encoding on the labels\n","lb = LabelBinarizer()\n","labels = lb.fit_transform(labels)\n","# only there are only two labels in the dataset, then we need to use\n","# Keras/TensorFlow's utility function as well\n","if len(lb.classes_) == 2: # Mặt định 2 chiều\n","\tlabels = to_categorical(labels)"],"metadata":{"id":"miUcX5EVdWe0","executionInfo":{"status":"aborted","timestamp":1687840066774,"user_tz":-420,"elapsed":7,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Phân chia dữ liệu**"],"metadata":{"id":"68oxDLqFdXjk"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import os\n","\n","split = train_test_split(data, labels, bboxes, imagePaths,test_size=0.20, random_state=42)\n","# unpack the data split\n","(trainImages, testImages) = split[:2]\n","(trainLabels, testLabels) = split[2:4]\n","(trainBBoxes, testBBoxes) = split[4:6]\n","(trainPaths, testPaths) = split[6:]\n","# write the testing image paths to disk so that we can use then\n","# when evaluating/testing our object detector\n","print(\"[INFO] saving testing image paths...\")\n","\n","# Check if the file exists before trying to remove it\n","if os.path.exists(TEST_PATHS):\n","    os.remove(TEST_PATHS)\n","\n","f = open(TEST_PATHS, \"w\")\n","f.write(\"\\n\".join(testPaths))\n","f.close()"],"metadata":{"id":"fPlnIr3ZdY1L","executionInfo":{"status":"aborted","timestamp":1687840066775,"user_tz":-420,"elapsed":8,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Xây dựng mô hình**"],"metadata":{"id":"fb00pWJbdaT2"}},{"cell_type":"code","source":["model = []\n","\n","# load the VGG16 network, ensuring the head FC layers are left off\n","vgg = InceptionV3(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n","vgg.trainable = False # Không lấy hàm phân loại VGG19\n","flatten = vgg.output\n","flatten = Flatten()(flatten)\n","\n","# construct a fully-connected layer header to output the predicted\n","# bounding box coordinates\n","bboxHead = Dense(128, activation=\"relu\")(flatten)\n","bboxHead = Dense(64, activation=\"relu\")(bboxHead)\n","bboxHead = Dense(32, activation=\"relu\")(bboxHead)\n","bboxHead = Dense(4, activation=\"sigmoid\",name=\"bounding_box\")(bboxHead)\n","\n","# construct a second fully-connected layer head, this one to predict\n","# the class label\n","softmaxHead = Dense(512, activation=\"relu\")(flatten)\n","softmaxHead = Dropout(0.5)(softmaxHead)\n","softmaxHead = Dense(512, activation=\"relu\")(softmaxHead)\n","softmaxHead = Dropout(0.5)(softmaxHead)\n","softmaxHead = Dense(len(lb.classes_), activation=\"softmax\",name=\"class_label\")(softmaxHead)\n","# put together our model which accept an input image and then output\n","# bounding box coordinates and a class label\n","\n","# Kiến tạo mô hình\n","model = Model(inputs=vgg.input,outputs=(bboxHead, softmaxHead))"],"metadata":{"id":"9L0CwiRDdcBG","executionInfo":{"status":"aborted","timestamp":1687840066775,"user_tz":-420,"elapsed":8,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Xác định loss và compile mô hình**"],"metadata":{"id":"UBq9z5k-ddtP"}},{"cell_type":"code","source":["losses = {\"class_label\": \"categorical_crossentropy\",\"bounding_box\": \"mean_squared_error\",}\n","lossWeights = {\t\"class_label\": 1.0,\t\"bounding_box\": 1.0}\n","# summary\n","opt = Adam(lr=INIT_LR)\n","model.compile(loss=losses, optimizer=opt, metrics=[\"accuracy\"], loss_weights=lossWeights)\n","print(model.summary())"],"metadata":{"id":"Ek05uyayde2I","executionInfo":{"status":"aborted","timestamp":1687840066775,"user_tz":-420,"elapsed":7,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Huấn luyện mô hình**"],"metadata":{"id":"Xi1CGaPxdhZQ"}},{"cell_type":"code","source":["trainTargets = {\"class_label\": trainLabels,\"bounding_box\": trainBBoxes}\n","testTargets = {\t\"class_label\": testLabels,\"bounding_box\": testBBoxes}"],"metadata":{"id":"2TsgdgvMdf3z","executionInfo":{"status":"aborted","timestamp":1687840066776,"user_tz":-420,"elapsed":8,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"[INFO] training model...\")\n","H = model.fit(\n","\ttrainImages, trainTargets,\n","\tvalidation_data=(testImages, testTargets),\n","\tbatch_size=BATCH_SIZE,\n","\tepochs=NUM_EPOCHS,\n","\tverbose=1)\n","\n","# serialize the model to disk - Save model\n","print(\"[INFO] saving object detector model...\")\n","model.save(MODEL_PATH, save_format=\"h5\")\n","\n","# serialize the label binarizer to disk\n","print(\"[INFO] saving label binarizer...\")\n","f = open(LB_PATH, \"wb\")\n","f.write(pickle.dumps(lb))\n","f.close()"],"metadata":{"id":"UUlOKdLidmzC","executionInfo":{"status":"aborted","timestamp":1687840066776,"user_tz":-420,"elapsed":8,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Hiển thị loss và accuracy**"],"metadata":{"id":"uY9gx_i_dof_"}},{"cell_type":"code","source":["# plot the total loss, label loss, and bounding box loss\n","lossNames = [\"loss\", \"class_label_loss\", \"bounding_box_loss\"]\n","N = np.arange(0, NUM_EPOCHS)\n","plt.style.use(\"ggplot\")\n","(fig, ax) = plt.subplots(3, 1, figsize=(13, 13))\n","# loop over the loss names\n","for (i, l) in enumerate(lossNames):\n","\t# plot the loss for both the training and validation data\n","\ttitle = \"Loss for {}\".format(l) if l != \"loss\" else \"Total loss\"\n","\tax[i].set_title(title)\n","\tax[i].set_xlabel(\"Epoch #\")\n","\tax[i].set_ylabel(\"Loss\")\n","\tax[i].plot(N, H.history[l], label=l)\n","\tax[i].plot(N, H.history[\"val_\" + l], label=\"val_\" + l)\n","\tax[i].legend()\n","\n","# save the losses figure and create a new figure for the accuracies\n","# Tạo ra thư mục nếu nó chưa tồn tại\n","if not os.path.exists(PLOTS_PATH):\n","    os.makedirs(PLOTS_PATH)\n","\n","plt.tight_layout()\n","plotPath = os.path.sep.join([PLOTS_PATH, \"losses.png\"])\n","plt.savefig(plotPath)\n","# plt.close()"],"metadata":{"id":"vFG8w_SGdqDY","executionInfo":{"status":"aborted","timestamp":1687840066776,"user_tz":-420,"elapsed":7,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a new figure for the accuracies\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(N, H.history[\"class_label_accuracy\"],\n","\tlabel=\"class_label_train_acc\")\n","plt.plot(N, H.history[\"val_class_label_accuracy\"],\n","\tlabel=\"val_class_label_acc\")\n","plt.title(\"Class Label Accuracy\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend(loc=\"lower left\")\n","\n","# save the accuracies plot\n","if not os.path.exists(PLOTS_PATH):\n","    os.makedirs(PLOTS_PATH)\n","\n","plotPath = os.path.sep.join([PLOTS_PATH, \"accs.png\"])\n","plt.savefig(plotPath)"],"metadata":{"id":"-aXKy4kMdrHb","executionInfo":{"status":"aborted","timestamp":1687840066776,"user_tz":-420,"elapsed":7,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get class names from the LabelBinarizer\n","class_names = lb.classes_\n","\n","# Create the confusion matrix as before\n","Y_pred = model.predict(testImages)\n","Y_pred_classes = np.argmax(Y_pred[1], axis=1)\n","Y_true = np.argmax(testTargets[\"class_label\"], axis=1)\n","confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n","confusion_mtx_percent = confusion_mtx.astype('float') / confusion_mtx.sum(axis=1)[:, np.newaxis]\n","\n","# When calling heatmap, pass class names to the xticklabels and yticklabels arguments\n","plt.figure(figsize=(15,10))\n","sns.heatmap(confusion_mtx_percent, annot=True, fmt='.2%', cmap='Blues',\n","            xticklabels=class_names, yticklabels=class_names)\n","plt.xlabel('Predicted')\n","plt.ylabel('Truth')\n","plt.show()"],"metadata":{"id":"bgoAW4NzySrc","executionInfo":{"status":"aborted","timestamp":1687840066777,"user_tz":-420,"elapsed":8,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **KIỂM THỬ MÔ HÌNH**"],"metadata":{"id":"F8-WFaeGds0E"}},{"cell_type":"markdown","source":["## **Import thư viện**"],"metadata":{"id":"v8S9OOYBdvm3"}},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","# import the necessary packages\n","from tensorflow.keras.preprocessing.image import img_to_array\n","from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","import mimetypes\n","import argparse\n","import imutils\n","import pickle\n","import cv2\n","import os"],"metadata":{"id":"oB-DUf2Zdve5","executionInfo":{"status":"aborted","timestamp":1687840066777,"user_tz":-420,"elapsed":8,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Load mô hình**"],"metadata":{"id":"i9HR2mgwdzUS"}},{"cell_type":"code","source":["print(\"[INFO] loading object detector...\")\n","model = load_model(MODEL_PATH)"],"metadata":{"id":"pyJ2QSNXdzKv","executionInfo":{"status":"aborted","timestamp":1687840066777,"user_tz":-420,"elapsed":8,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Dự đoán hình ảnh bất kỳ**"],"metadata":{"id":"tJ09y19Nd1PX"}},{"cell_type":"code","source":["image = load_img(imagePath, target_size=(224, 224))\n","image = img_to_array(image) / 255.0\n","image = np.expand_dims(image, axis=0)\n","# make bounding box predictions on the input image\n","(boxPreds, labelPreds) = model.predict(image)\n","(startX, startY, endX, endY) = boxPreds[0]\n","# load the input image (in OpenCV format), resize it such that it\n","# fits on our screen, and grab its dimensions\n","image = cv2.imread(imagePath)\n","image = imutils.resize(image, width=600)\n","(h, w) = image.shape[:2]\n","# scale the predicted bounding box coordinates based on the image\n","# dimensions\n","startX = int(startX * w)\n","startY = int(startY * h)\n","endX = int(endX * w)\n","endY = int(endY * h)\n","# Draw the predicted bounding box\n","i = np.argmax(labelPreds, axis=1)\n","label = lb.classes_[i][0]\n","y = startY - 10 if startY - 10 > 10 else startY + 10\n","cv2.putText(image, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX,0.65, (0, 255, 0), 2)\n","cv2.rectangle(image, (startX, startY), (endX, endY),(0, 255, 0), 2)\n","# Hiển thị hình ảnh\n","cv2_imshow(image)"],"metadata":{"id":"jVTQbek3dvEQ","executionInfo":{"status":"aborted","timestamp":1687840066778,"user_tz":-420,"elapsed":9,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tải mô hình từ đường dẫn đã cho\n","print(\"[INFO] đang tải mô hình...\")\n","model = load_model(MODEL_PATH)\n","\n","# Tải và tiền xử lý hình ảnh\n","image = load_img(imagePath, target_size=(224, 224))\n","image = img_to_array(image) / 255.0\n","image = np.expand_dims(image, axis=0)\n","\n","# Dự đoán tọa độ bounding box và nhãn cho hình ảnh đầu vào\n","(boxPreds, labelPreds) = model.predict(image)\n","(startX, startY, endX, endY) = boxPreds[0]\n","\n","# Tải hình ảnh đầu vào (định dạng OpenCV), thay đổi kích thước sao cho\n","# nó phù hợp với màn hình, và lấy các chiều của nó\n","image = cv2.imread(imagePath)\n","image = imutils.resize(image, width=600)\n","(h, w) = image.shape[:2]\n","\n","# Chuyển đổi tọa độ bounding box dự đoán dựa trên chiều của hình ảnh\n","startX = int(startX * w)\n","startY = int(startY * h)\n","endX = int(endX * w)\n","endY = int(endY * h)\n","\n","# Lấy nhãn với xác suất cao nhất\n","i = np.argmax(labelPreds, axis=1)\n","label = lb.classes_[i][0]\n","\n","# Lấy xác suất cho nhãn này\n","prob = np.max(labelPreds, axis=1)\n","\n","# Định dạng là phần trăm và thêm vào nhãn\n","label = \"{}: {:.2f}%\".format(label, prob[0] * 100)  # Chỉnh sửa ở đây\n","\n","# Vẽ bounding box và nhãn dự đoán lên hình ảnh\n","cv2.putText(image, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX,0.65, (0, 255, 0), 2)\n","cv2.rectangle(image, (startX, startY), (endX, endY),(0, 255, 0), 2)\n","\n","# Hiển thị hình ảnh\n","cv2_imshow(image)\n"],"metadata":{"id":"EuCbHhgQzfj0","executionInfo":{"status":"aborted","timestamp":1687840066779,"user_tz":-420,"elapsed":10,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_path_test = [\"/content/drive/MyDrive/NLMH_Thuchanh/Week15_Thuchanh8/21022008/Q0Eimage.png\",\n","                \"/content/drive/MyDrive/NLMH_Thuchanh/Week15_Thuchanh8/21022008/tehimage.png\"]"],"metadata":{"id":"nS5Zwn7pz8Zm","executionInfo":{"status":"aborted","timestamp":1687840066779,"user_tz":-420,"elapsed":10,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tải mô hình từ đường dẫn đã cho\n","print(\"[INFO] đang tải mô hình...\")\n","model = load_model(MODEL_PATH)\n","\n","# Duyệt qua mỗi hình ảnh trong danh sách\n","for imagePath in img_path_test:\n","    # Tải và tiền xử lý hình ảnh\n","    image = load_img(imagePath, target_size=(224, 224))\n","    image = img_to_array(image) / 255.0\n","    image = np.expand_dims(image, axis=0)\n","\n","    # Dự đoán tọa độ bounding box và nhãn cho hình ảnh đầu vào\n","    (boxPreds, labelPreds) = model.predict(image)\n","    (startX, startY, endX, endY) = boxPreds[0]\n","\n","    # Tải hình ảnh đầu vào (định dạng OpenCV), thay đổi kích thước sao cho\n","    # nó phù hợp với màn hình, và lấy các chiều của nó\n","    image = cv2.imread(imagePath)\n","    image = imutils.resize(image, width=600)\n","    (h, w) = image.shape[:2]\n","\n","    # Chuyển đổi tọa độ bounding box dự đoán dựa trên chiều của hình ảnh\n","    startX = int(startX * w)\n","    startY = int(startY * h)\n","    endX = int(endX * w)\n","    endY = int(endY * h)\n","\n","    # Lấy nhãn với xác suất cao nhất\n","    i = np.argmax(labelPreds, axis=1)\n","    label = lb.classes_[i][0]\n","\n","    # Lấy xác suất cho nhãn này\n","    prob = np.max(labelPreds, axis=1)\n","\n","    # Định dạng là phần trăm và thêm vào nhãn\n","    label = \"{}: {:.2f}%\".format(label, prob[0] * 100)\n","\n","    # Vẽ bounding box và nhãn dự đoán lên hình ảnh\n","    cv2.putText(image, label, (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX,0.65, (0, 255, 0), 2)\n","    cv2.rectangle(image, (startX, startY), (endX, endY),(0, 255, 0), 2)\n","\n","    # Hiển thị hình ảnh\n","    cv2_imshow(image)\n"],"metadata":{"id":"sm9z5SnAz6cP","executionInfo":{"status":"aborted","timestamp":1687840066779,"user_tz":-420,"elapsed":10,"user":{"displayName":"Huu Tho Nguyen","userId":"16998784333720908057"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**1. Thực hiện training với tập dữ liệu đã được gán nhãn ở phần 1 với số lượng epochs là 50**\n","\n","**2. Thực hiện vẽ biểu đồ loss - accuracy - confusion matrix cho mạng VGG19**\n","\n","**3. Thực hiện training với tập dữ liệu với mạng InceptionV3, epochs 50 và lưu tên mô hình sau khi huấn luyện là MSSV_Incep.h5**\n","\n","**4. Sử dụng hình ảnh bên dưới và thực hiện dự đoán với 2 mô hình vừa đào tạo**\n","\n","**5. Hiển thị label và chỉ số độ chính xác trên hình ảnh dự đoán như ảnh bên dưới với 2 hình ở câu 4**"],"metadata":{"id":"Gy3UwulCd6y-"}}]}